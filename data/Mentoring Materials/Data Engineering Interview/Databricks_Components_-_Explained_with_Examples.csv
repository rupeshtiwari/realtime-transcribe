Component,What it is,What it's used for,Tools,When to Use
Delta Lake,"Open-source storage layer that brings ACID transactions to data lakes (e.g., S3, ADLS).","Used to create reliable data lakes with support for schema evolution, time travel, and concurrent writes.","Tools: Apache Spark, Databricks, AWS S3, Azure Data Lake.",Use when you need transaction guarantees on large-scale data and fast query performance.
Delta Live Tables (DLT),Databricks-native framework for declarative data pipelines using SQL or Python.,"Used to build ETL pipelines that are automatically monitored, versioned, and recoverable.","Tools: Databricks Workflows, Delta Lake, SQL, Python.",Use when you want to reduce boilerplate in ETL and gain automatic observability and recovery.
Unity Catalog,"Databricksâ€™ unified governance layer for managing data, metadata, and permissions across all assets.","Provides centralized fine-grained access control, audit logging, and cataloging of data and models.","Tools: Databricks Unity Catalog, Delta Lake, DBFS, MLflow integration.",Use when multiple teams share data and you need column/row-level security and centralized governance.
MLflow,"Open-source platform for managing the ML lifecycle: experiments, models, and deployments.","Tracks experiments, stores models (with versions), and deploys across environments (e.g., SageMaker, Databricks).","Tools: MLflow Tracking, MLflow Registry, MLflow Projects.","Use when you're building ML pipelines and need reproducibility, experiment tracking, and model governance."
