Step,Clarification Questions,Mentor's Answer,Examples / Tools / Notes
Capture,"['What is the incoming event volume per second?', 'Is it real-time (event-time) or batch (ingest-time)?', 'Are events pushed or pulled?', 'Is ordering important per user/session?']","We receive ~10K click events/sec. Use Apache Kafka (push model) with topics per app, partitioned by `user_id` or `region` for parallelism and ordering.","Apache Kafka, Kinesis, REST API (Airbyte). Partitioning by user_id = preserves order. Avro format preferred for schemas."
Audit & Validate,"['What data quality rules must be enforced?', 'Are schema changes expected frequently?', 'How do we handle invalid records?', 'Do we need validation at ingest and after loading?']",Use PyDantic to validate JSON structure at ingest; Great Expectations for schema + null checks post-load. Send invalids to Kafka DLQ.,"Use: PyDantic (Python), Great Expectations, Kafka DLQ. Check: null %, column type, row count. Run GE validations as Airflow task."
Model,"['Should the model support historical changes (SCD2)?', 'What metrics are needed? Sessions, clicks, journeys?', 'Are we modeling for BI (denormalized) or storage optimized?', 'Is there need for dimensional modeling (star/snowflake)?']","Use Star Schema for BI, with surrogate keys. Clickstream facts joined with user and session dimensions. Track changes via SCD Type 2 in dim_user.","Fact: fact_clicks(user_sk, page_sk, ts, click_type, session_id). Dim_user: surrogate_key, user_id, start_date, end_date, is_current. Tools: dbt, Snowflake, BigQuery."
Persist,"['Do we need ACID, schema evolution, or time travel?', 'What’s the retention period for raw data?', 'Do we query from Spark or SQL engines?', 'Do we need incremental loads?']","Store raw events in Parquet in S3. Curated data in Delta Lake with ACID and schema enforcement. Partition by `dt`, Z-order by session_id for query speed.","Formats: Delta Lake, Apache Iceberg. Tools: Spark, AWS Glue, Hudi. Z-Ordering: speeds WHERE session_id. Raw retention: 30–90 days."
Expose,"['Are real-time dashboards required?', 'Who consumes the data — BI analysts, apps, ML?', 'What are latency requirements (<1 sec or hourly)?', 'Are APIs needed to access processed data?']",Use Apache Druid for real-time dashboards (<1s). Load batch data to Snowflake via dbt for analysts. Provide REST APIs if needed.,Druid for real-time OLAP. dbt models in Snowflake. API layer: GraphQL or FastAPI to expose clean tables.
Resilience,"['What failure scenarios are critical to guard against?', 'Should we reprocess failed data or skip?', 'What’s the tolerance for delayed data?', 'Are retries and checkpoints required?']",Use Spark checkpointing to resume on failure. Airflow retries with exponential backoff. Spark `badRecordsPath` isolates corrupt data. Build idempotent DAGs.,"Techniques: Spark Structured Streaming, retries (max_tries=3), DAG idempotency via {{ ds }} param, DLQ + alerting."
Security,"['Is clickstream data considered PII (e.g., user_id)?', 'Do we need to mask or hash identifiers?', 'How do we manage access control to raw/curated layers?', 'Are we compliant with GDPR or CCPA?']",Encrypt data in transit (TLS) and at rest (S3 + KMS). Use RBAC in Snowflake. Mask `user_id` using SHA-256 in curated data. Set IAM roles for lake access.,"Tools: AWS KMS, dbt column masking, IAM roles, Snowflake RBAC. Use column-level encryption if sensitive."
Observability,"['How will we detect data pipeline failures?', 'Do we track freshness, lineage, and volume drops?', 'Do we monitor schema changes over time?', 'Should we alert on processing delays?']",Use Airflow + Prometheus for task-level metrics. Add Grafana alerts on SLA misses or row count drops. Log schema versions using Iceberg’s manifest files.,"Tools: OpenLineage, Great Expectations logs, Databand. Metrics: DAG duration, freshness, validation %, ingestion lag."
